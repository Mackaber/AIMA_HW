{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agents import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAgent(Agent):\n",
    "    def __init__(self, program=None):\n",
    "        super().__init__(program)\n",
    "        self.things = []\n",
    "        self.last_action = \"none\"\n",
    "        self.sign = \"a \"\n",
    "        \n",
    "    # Movements\n",
    "    \n",
    "    def moveRight(self):\n",
    "        self.location = (self.location[0] + 1, self.location[1])\n",
    "\n",
    "    def moveLeft(self):\n",
    "        self.location = (self.location[0] - 1, self.location[1])\n",
    "\n",
    "    def moveDown(self):\n",
    "        self.location = (self.location[0], self.location[1] + 1)\n",
    "\n",
    "    def moveUp(self):\n",
    "        self.location = (self.location[0], self.location[1] - 1)\n",
    "\n",
    "    def noOp(self):\n",
    "        pass\n",
    "    \n",
    "    # Agent State\n",
    "\n",
    "    def add_perecpted_things(self, things):\n",
    "        for thing in things:\n",
    "            self.things.append(thing)\n",
    "\n",
    "    def thing_at(self, location):\n",
    "        for thing in self.things:\n",
    "            if thing.location == location:\n",
    "                return thing.sign\n",
    "        return \"? \"\n",
    "\n",
    "    def show_state(self):\n",
    "        print('\\n' + \" Agent's internal state: \" + '\\n')\n",
    "        print(\"  0 1 2 3 4 5 \")\n",
    "        for y in range(6):\n",
    "            print(y, end=' ')\n",
    "            for x in range(6):\n",
    "                print(self.thing_at((x, y)), end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AThing(Thing):\n",
    "    def __init__(self,value,sign,location=(0,0)):\n",
    "        self.location = location\n",
    "        self.value = value\n",
    "        self.sign = sign\n",
    "\n",
    "class Nothing(AThing):\n",
    "    def __init__(self,location=(0,0)):\n",
    "        super().__init__(-1,\"- \",location)\n",
    "\n",
    "class Outside(AThing):\n",
    "    def __init__(self,location=(0,0)):\n",
    "        super().__init__(-10,\"o \",location)\n",
    "\n",
    "class Wall(AThing):\n",
    "    def __init__(self):\n",
    "        super().__init__(-5,\"w \")\n",
    "\n",
    "class Treasure_gold(AThing):\n",
    "    def __init__(self):\n",
    "        super().__init__(20,\"g \")\n",
    "\n",
    "class Treasure_diamond(AThing):\n",
    "    def __init__(self):\n",
    "        super().__init__(50,\"d \")\n",
    "\n",
    "class Snake_trap(AThing):\n",
    "    def __init__(self,):\n",
    "        super().__init__(-20,\"s \")\n",
    "\n",
    "class Hole_trap(AThing):\n",
    "    def __init__(self):\n",
    "        super().__init__(-15,\"h \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid(percepts):\n",
    "    percs = []\n",
    "    for x in percepts:\n",
    "        if isinstance(x,Outside) or isinstance(x,Wall):\n",
    "            pass\n",
    "        else:\n",
    "            percs.append(x)\n",
    "    return percs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEnv(Environment):    \n",
    "        \n",
    "    # Things displayed (different from thing_at, because this includes the agent)\n",
    "    def disp_thing_at(self,agent,location):\n",
    "        if agent.location == location:\n",
    "            return agent\n",
    "        elif location[0] < 0 or location[1] < 0 or location[0] > 5 or location[1] > 5:\n",
    "            return Outside(location)\n",
    "        elif self.list_things_at(location):\n",
    "            return self.list_things_at(location)[0]\n",
    "        else:\n",
    "            return Nothing(location)\n",
    "        \n",
    "    # Different from disp_thing_at, because this excludes the agent)    \n",
    "    def thing_at(self, agent, location):\n",
    "        if location[0] < 0 or location[1] < 0 or location[0] > 5 or location[1] > 5:\n",
    "            return Outside(location)\n",
    "        elif self.list_things_at(location):\n",
    "            # This returns the thing that isn't the agent it doesn't found anything returns nothing\n",
    "            if agent in self.list_things_at(location):\n",
    "                for t in self.list_things_at(location):\n",
    "                    if not isinstance(t,MAgent):\n",
    "                        return t\n",
    "                return Nothing(location)\n",
    "            else:\n",
    "                return self.list_things_at(location)[0]\n",
    "        else:\n",
    "            return Nothing(location)\n",
    "\n",
    "    def percept(self, agent):\n",
    "        location = agent.location\n",
    "\n",
    "        here = self.thing_at(agent, location)\n",
    "        left = self.thing_at(agent, (location[0] - 1, location[1]))  # Left\n",
    "        down = self.thing_at(agent, (location[0], location[1] + 1))  # Down\n",
    "        right = self.thing_at(agent, (location[0] + 1, location[1]))  # Right\n",
    "        up = self.thing_at(agent, (location[0], location[1] - 1))  # Up\n",
    "\n",
    "        here.position = \"nop\"\n",
    "        left.position = \"left\"\n",
    "        down.position = \"down\"\n",
    "        right.position = \"right\"\n",
    "        up.position = \"up\"\n",
    "\n",
    "        percepted_things = [here, left, down, up, right]\n",
    "        \n",
    "        print(\"<BEFORE THE ACTION>\")\n",
    "        self.show_grid(agent)\n",
    "        agent.add_perecpted_things(percepted_things)\n",
    "        agent.show_state()\n",
    "        print(\"Agent performance: \",agent.performance)\n",
    "        print(\"<AGENT PROGRAM>\")\n",
    "        print(\"Percept: ({},({},{},{},{},{}))\".format(location, percepted_things[0].sign, percepted_things[1].sign, percepted_things[2].sign, percepted_things[3].sign, percepted_things[4].sign))\n",
    "        return {\"location\": location, \"positions\": remove_invalid(percepted_things) } #agent.things\n",
    "\n",
    "\n",
    "    def execute_action(self, agent, action):\n",
    "        print(\"Action: \",action)\n",
    "        if action == \"nop\":\n",
    "            pass\n",
    "        else:\n",
    "            agent.performance -= 1\n",
    "            if action == \"right\":\n",
    "                agent.moveRight()\n",
    "            elif action == \"left\":\n",
    "                agent.moveLeft()\n",
    "            elif action == \"up\":\n",
    "                agent.moveUp()\n",
    "            elif action == \"down\":\n",
    "                agent.moveDown()\n",
    "            elif action == \"nop\":\n",
    "                agent.noOp\n",
    "\n",
    "        location = agent.location\n",
    "        thing = self.thing_at(agent,location)\n",
    "        \n",
    "        # Discriminate path already covered to prevent loops\n",
    "        if isinstance(thing,Nothing):\n",
    "            self.add_thing(Nothing(),location)\n",
    "        thing.value = thing.value - 1\n",
    "        \n",
    "        if isinstance(thing, Treasure_gold) or isinstance(thing, Treasure_diamond):\n",
    "            self.delete_thing(thing)\n",
    "            agent.performance += thing.value\n",
    "\n",
    "        if isinstance(thing, Snake_trap) or isinstance(thing, Hole_trap):\n",
    "            agent.performance += thing.value\n",
    "            \n",
    "        agent.last_action = action\n",
    "        print(\"<AFTER THE ACTION>\")\n",
    "        self.show_grid(agent)\n",
    "        agent.show_state()\n",
    "        print(\"Agent performance: \",agent.performance)\n",
    "        print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "    def show_grid(self,agent):\n",
    "        print(\" The environment content: \" + '\\n')\n",
    "        print(\"  0 1 2 3 4 5 \")\n",
    "        for y in range(6):\n",
    "            print(y, end=' ')\n",
    "            for x in range(6):\n",
    "                print(self.disp_thing_at(agent,(x, y)).sign, end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def areEquals(e):\n",
    "    if e[0] == e[1] == e[2] == e[3]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def program(percepts):\n",
    "    \n",
    "    if areEquals(percepts[\"positions\"]):\n",
    "        best_choice = random.choice(percepts[\"positions\"])\n",
    "        return best_choice\n",
    "    else:\n",
    "        xx = max(percepts[\"positions\"], key=lambda x: x.value).position\n",
    "        best_choice = xx\n",
    "        return best_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 a - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d ? ? ? ? ? \n4 ? ? ? ? ? ? \n5 ? ? ? ? ? ? \nAgent performance:  0\n<AGENT PROGRAM>\nPercept: ((0, 2),(- ,o ,d ,h ,- ))\nAction:  down\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 a w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d ? ? ? ? ? \n4 ? ? ? ? ? ? \n5 ? ? ? ? ? ? \nAgent performance:  48\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 a w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - ? ? ? ? ? \n5 ? ? ? ? ? ? \nAgent performance:  48\n<AGENT PROGRAM>\nPercept: ((0, 3),(- ,o ,- ,- ,w ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 a w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - ? ? ? ? ? \n5 ? ? ? ? ? ? \nAgent performance:  48\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 a w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - ? ? ? ? ? \n5 ? ? ? ? ? ? \nAgent performance:  48\n<AGENT PROGRAM>\nPercept: ((0, 3),(- ,o ,- ,- ,w ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 a w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - ? ? ? ? ? \n5 ? ? ? ? ? ? \nAgent performance:  48\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 a w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - ? ? ? ? ? \n5 ? ? ? ? ? ? \nAgent performance:  48\n<AGENT PROGRAM>\nPercept: ((0, 3),(- ,o ,- ,- ,w ))\nAction:  down\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 a - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - ? ? ? ? ? \n5 ? ? ? ? ? ? \nAgent performance:  47\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 a - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - - ? ? ? ? \n5 s ? ? ? ? ? \nAgent performance:  47\n<AGENT PROGRAM>\nPercept: ((0, 4),(- ,o ,s ,- ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 a - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - - ? ? ? ? \n5 s ? ? ? ? ? \nAgent performance:  47\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 a - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - - ? ? ? ? \n5 s ? ? ? ? ? \nAgent performance:  47\n<AGENT PROGRAM>\nPercept: ((0, 4),(- ,o ,s ,- ,- ))\nAction:  right\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - a - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - - ? ? ? ? \n5 s ? ? ? ? ? \nAgent performance:  46\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - a - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - - - ? "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ? \n5 s - ? ? ? ? \nAgent performance:  46\n<AGENT PROGRAM>\nPercept: ((1, 4),(- ,- ,- ,w ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - a - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - - - ? ? ? \n5 s - ? ? ? ? \nAgent performance:  46\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - a - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - - - ? ? ? \n5 s - ? ? ? ? \nAgent performance:  46\n<AGENT PROGRAM>\nPercept: ((1, 4),(- ,- ,- ,w ,- ))\nAction:  down\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s a - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - - - ? ? ? \n5 s - ? ? ? ? \nAgent performance:  45\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s a - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - - - ? ? ? \n5 s - - ? ? ? \nAgent performance:  45\n<AGENT PROGRAM>\nPercept: ((1, 5),(- ,s ,o ,- ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s a - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - - - ? "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ? \n5 s - - ? ? ? \nAgent performance:  45\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s a - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - - - ? ? ? \n5 s - - ? ? ? \nAgent performance:  45\n<AGENT PROGRAM>\nPercept: ((1, 5),(- ,s ,o ,- ,- ))\nAction:  right\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - a - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - - - ? ? ? \n5 s - - ? ? ? \nAgent performance:  44\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - a - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - - - ? ? ? \n5 s - - - ? ? \nAgent performance:  44\n<AGENT PROGRAM>\nPercept: ((2, 5),(- ,- ,o ,- ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - a - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - - - ? ? ? \n5 s - - - ? ? \nAgent performance:  44\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - a - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - - - ? ? ? \n5 s - - - ? ? \nAgent performance:  44\n<AGENT PROGRAM>\nPercept: ((2, 5),(- ,- ,o ,- ,- ))\nAction:  up\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - a h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w ? ? ? ? \n4 - - - ? ? ? \n5 s - - - ? ? \nAgent performance:  43\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - a h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  43\n<AGENT PROGRAM>\nPercept: ((2, 4),(- ,- ,- ,w ,h ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - a h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  43\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - a h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  43\n<AGENT PROGRAM>\nPercept: ((2, 4),(- ,- ,- ,w ,h ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - a h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - ? ? \nAgent performance:  43\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - a h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  43\n<AGENT PROGRAM>\nPercept: ((2, 4),(- ,- ,- ,w ,h ))\nAction:  left\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - a - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  42\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - a - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  42\n<AGENT PROGRAM>\nPercept: ((1, 4),(- ,- ,- ,w ,- ))\nAction:  left\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 a - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  41\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 a - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  41\n<AGENT PROGRAM>\nPercept: ((0, 4),(- ,o ,s ,- ,- ))\nAction:  up\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 a w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  40\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 a w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  40\n<AGENT PROGRAM>\nPercept: ((0, 3),(- ,o ,- ,- ,w ))\nAction:  up\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 a - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  39\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 a - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  39\n<AGENT PROGRAM>\nPercept: ((0, 2),(- ,o ,- ,h ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 a - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nAgent performance:  39\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 a - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  39\n<AGENT PROGRAM>\nPercept: ((0, 2),(- ,o ,- ,h ,- ))\nAction:  right\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - a - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h ? ? ? ? ? \n2 - - ? ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  38\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - a - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h - ? ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  38\n<AGENT PROGRAM>\nPercept: ((1, 2),(- ,- ,w ,- ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - a - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h - ? ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  38\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - a - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h - ? ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  38\n<AGENT PROGRAM>\nPercept: ((1, 2),(- ,- ,w ,- ,- ))\nAction:  up\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h a s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? ? ? ? ? ? \n1 h - ? ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  37\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h a s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? - ? ? ? ? \n1 h - s ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  37\n<AGENT PROGRAM>\nPercept: ((1, 1),(- ,h ,- ,- ,s ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h a s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? - ? ? ? ? \n1 h - s ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  37\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h a s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? - ? ? ? ? \n1 h - s ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  37\n<AGENT PROGRAM>\nPercept: ((1, 1),(- ,h ,- ,- ,s ))\nAction:  up\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 g a - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 ? - ? ? ? ? \n1 h - s ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  36\n----------------------------------------------------------------------\n<BEFORE THE ACTION>"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n The environment content: \n\n  0 1 2 3 4 5 \n0 g a - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - ? ? ? \n1 h - s ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  36\n<AGENT PROGRAM>\nPercept: ((1, 0),(- ,g ,- ,o ,- ))\nAction:  left\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 a - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - ? ? ? \n1 h - s ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  54\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 a - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - ? ? ? \n1 h - s ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  54\n<AGENT PROGRAM>\nPercept: ((0, 0),(- ,o ,h ,o ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 a - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - ? ? ? \n1 h - s ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  54\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 a - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - ? ? ? \n1 h - s ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  54\n<AGENT PROGRAM>\nPercept: ((0, 0),(- ,o ,h ,o ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 a - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - ? ? ? \n1 h - s ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  54\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 a - - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - ? ? ? \n1 h - s ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  54\n<AGENT PROGRAM>\nPercept: ((0, 0),(- ,o ,h ,o ,- ))\nAction:  right\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - a - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - ? ? ? \n1 h - s ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  53\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - a - - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - ? ? ? \n1 h - s ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  53\n<AGENT PROGRAM>\nPercept: ((1, 0),(- ,- ,- ,o ,- ))\nAction:  right\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - a - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - ? ? ? \n1 h - s ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  52\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- a - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - ? ? \n1 h - s ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  52\n<AGENT PROGRAM>\nPercept: ((2, 0),(- ,- ,s ,o ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - a - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - ? ? \n1 h - s ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  52\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - a - - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - ? ? \n1 h - s ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  52\n<AGENT PROGRAM>\nPercept: ((2, 0),(- ,- ,s ,o ,- ))\nAction:  right\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - a - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - ? ? \n1 h - s ? ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  51\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - a - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - ? \n1 h - s w ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  51\n<AGENT PROGRAM>\nPercept: ((3, 0),(- ,- ,w ,o ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - a - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - ? \n1 h - s w ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  51\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - a - - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - ? \n1 h - s w ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  51\n<AGENT PROGRAM>\nPercept: ((3, 0),(- ,- ,w ,o ,- ))\nAction:  right\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - a - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - ? \n1 h - s w ? ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  50\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - a - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  50\n<AGENT PROGRAM>\nPercept: ((4, 0),(- ,- ,- ,o ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - a - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  50\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - a - \n1 h - s w - - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  50\n<AGENT PROGRAM>\nPercept: ((4, 0),(- ,- ,- ,o ,- ))\nAction:  down\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w a - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - ? \n2 - - - ? ? ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  49\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w a - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - ? - ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  49\n<AGENT PROGRAM>\nPercept: ((4, 1),(- ,w ,- ,- ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w a - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - ? - ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  49\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w a - \n2 - - - w - d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - ? - ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  49\n<AGENT PROGRAM>\nPercept: ((4, 1),(- ,w ,- ,- ,- ))\nAction:  down\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 h - s w - - \n2 - - - w a d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - ? - ? \n3 d w w ? ? ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  48\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w a d \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w ? - ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  48\n<AGENT PROGRAM>\nPercept: ((4, 2),(- ,w ,- ,- ,d ))\nAction:  right\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - a \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w ? - ? \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  96\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - a \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w ? - - \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  96\n<AGENT PROGRAM>\nPercept: ((5, 2),(- ,- ,- ,- ,o ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - a \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w ? - - \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  96\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - a \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w ? - - \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  96\n<AGENT PROGRAM>\nPercept: ((5, 2),(- ,- ,- ,- ,o ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - a \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w ? - - \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  96\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - a \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w ? - - \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  96\n<AGENT PROGRAM>\nPercept: ((5, 2),(- ,- ,- ,- ,o ))\nAction:  left\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w a - \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w ? - - \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  95\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w a - \n3 - w w - - - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w ? - - \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  95\n<AGENT PROGRAM>\nPercept: ((4, 2),(- ,w ,- ,- ,- ))\nAction:  down\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - a - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w ? - - \n4 - - - h ? ? \n5 s - - - ? ? \nAgent performance:  94\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - a - \n4 - - - h d - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d ? \n5 s - - - ? ? \nAgent performance:  94\n<AGENT PROGRAM>\nPercept: ((4, 3),(- ,- ,d ,- ,- ))\nAction:  down\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h a - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d ? \n5 s - - - ? ? \nAgent performance:  142\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h a - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - ? \nAgent performance:  142\n<AGENT PROGRAM>\nPercept: ((4, 4),(- ,h ,- ,- ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h a - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - ? \nAgent performance:  142\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - w - - \n3 - w w - - - \n4 - - - h a - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - ? \nAgent performance:  142\n<AGENT PROGRAM>\nPercept: ((4, 4),(- ,h ,- ,- ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h a - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - ? \nAgent performance:  142\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h a - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - ? \nAgent performance:  142\n<AGENT PROGRAM>\nPercept: ((4, 4),(- ,h ,- ,- ,- ))\nAction:  down\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - a - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - ? \nAgent performance:  141\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - a - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  141\n<AGENT PROGRAM>\nPercept: ((4, 5),(- ,- ,o ,- ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - a - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  141\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - a - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  141\n<AGENT PROGRAM>\nPercept: ((4, 5),(- ,- ,o ,- ,- ))\nAction:  left\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - a - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  140\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - a - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  140\n<AGENT PROGRAM>\nPercept: ((3, 5),(- ,- ,o ,h ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - a - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  140\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - a - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  140\n<AGENT PROGRAM>\nPercept: ((3, 5),(- ,- ,o ,h ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - a - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  140\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - a - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  140\n<AGENT PROGRAM>\nPercept: ((3, 5),(- ,- ,o ,h ,- ))\nAction:  left\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - a - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  139\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - a - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  139\n<AGENT PROGRAM>\nPercept: ((2, 5),(- ,- ,o ,- ,- ))\nAction:  left\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n3 - w w - - - \n4 - - - h - - \n5 s a - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  138\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s a - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  138\n<AGENT PROGRAM>\nPercept: ((1, 5),(- ,s ,o ,- ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s a - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  138\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s a - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  138\n<AGENT PROGRAM>\nPercept: ((1, 5),(- ,s ,o ,- ,- ))\nAction:  up\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - a - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  137\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - a - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  137\n<AGENT PROGRAM>\nPercept: ((1, 4),(- ,- ,- ,w ,- ))\nAction:  left\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 a - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  136\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 a - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  136\n<AGENT PROGRAM>\nPercept: ((0, 4),(- ,o ,s ,- ,- ))\nAction:  up\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 a w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  135\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 a w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  135\n<AGENT PROGRAM>\nPercept: ((0, 3),(- ,o ,- ,- ,w ))\nAction:  up\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 a - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  134\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 a - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  134\n<AGENT PROGRAM>\nPercept: ((0, 2),(- ,o ,- ,h ,- ))\nAction:  right\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - a - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  133\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - a - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  133\n<AGENT PROGRAM>\nPercept: ((1, 2),(- ,- ,w ,- ,- ))\nAction:  right\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - a w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  132\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - a w - - \n3 - w w - - - \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  132\n<AGENT PROGRAM>\nPercept: ((2, 2),(- ,- ,w ,s ,w ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - a w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  132\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - a w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  132\n<AGENT PROGRAM>\nPercept: ((2, 2),(- ,- ,w ,s ,w ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - a w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  132\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - a w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  132\n<AGENT PROGRAM>\nPercept: ((2, 2),(- ,- ,w ,s ,w ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - a w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  132\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - a w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  132\n<AGENT PROGRAM>\nPercept: ((2, 2),(- ,- ,w ,s ,w ))\nAction:  left\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - a - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  131\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - a - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  131\n<AGENT PROGRAM>\nPercept: ((1, 2),(- ,- ,w ,- ,- ))\nAction:  up\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h a s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  130\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h a s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  130\n<AGENT PROGRAM>\nPercept: ((1, 1),(- ,h ,- ,- ,s ))\nAction:  up\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - a - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  129\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - a - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  129\n<AGENT PROGRAM>\nPercept: ((1, 0),(- ,- ,- ,o ,- ))\nAction:  left\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 a - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  128\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 a - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  128\n<AGENT PROGRAM>\nPercept: ((0, 0),(- ,o ,h ,o ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 a - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  128\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 a - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  128\n<AGENT PROGRAM>\nPercept: ((0, 0),(- ,o ,h ,o ,- ))\nAction:  right\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - a - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  127\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - a - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  127\n<AGENT PROGRAM>\nPercept: ((1, 0),(- ,- ,- ,o ,- ))\nAction:  right\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - a - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  126\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - a - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  126\n<AGENT PROGRAM>\nPercept: ((2, 0),(- ,- ,s ,o ,- ))\nAction:  right\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - a - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  125\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - a - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  125\n<AGENT PROGRAM>\nPercept: ((3, 0),(- ,- ,w ,o ,- ))\nAction:  right\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - a - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  124\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - a - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  124\n<AGENT PROGRAM>\nPercept: ((4, 0),(- ,- ,- ,o ,- ))\nAction:  right\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - a \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  123\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - a \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  123\n<AGENT PROGRAM>\nPercept: ((5, 0),(- ,- ,- ,o ,o ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - a \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  123\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - a \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  123\n<AGENT PROGRAM>\nPercept: ((5, 0),(- ,- ,- ,o ,o ))\nAction:  down\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - a \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  122\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - a \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  122\n<AGENT PROGRAM>\nPercept: ((5, 1),(- ,- ,- ,- ,o ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - a \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  122\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - a \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  122\n<AGENT PROGRAM>\nPercept: ((5, 1),(- ,- ,- ,- ,o ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - a \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  122\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - a \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  122\n<AGENT PROGRAM>\nPercept: ((5, 1),(- ,- ,- ,- ,o ))\nAction:  left\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w a - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  121\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w a - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  121\n<AGENT PROGRAM>\nPercept: ((4, 1),(- ,w ,- ,- ,- ))\nAction:  down\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w a - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  120\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w a - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  120\n<AGENT PROGRAM>\nPercept: ((4, 2),(- ,w ,- ,- ,- ))\nAction:  down\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - a - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  119\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - a - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  119\n<AGENT PROGRAM>\nPercept: ((4, 3),(- ,- ,- ,- ,- ))\nAction:  left\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w a - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  118\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w a - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  118\n<AGENT PROGRAM>\nPercept: ((3, 3),(- ,w ,h ,w ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w a - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  118\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w a - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  118\n<AGENT PROGRAM>\nPercept: ((3, 3),(- ,w ,h ,w ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w a - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  118\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w a - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  118\n<AGENT PROGRAM>\nPercept: ((3, 3),(- ,w ,h ,w ,- ))\nAction:  right\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - a - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  117\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - a - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  117\n<AGENT PROGRAM>\nPercept: ((4, 3),(- ,- ,- ,- ,- ))\nAction:  right\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - a \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  116\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - a \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  116\n<AGENT PROGRAM>\nPercept: ((5, 3),(- ,- ,- ,- ,o ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - a \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  116\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - a \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  116\n<AGENT PROGRAM>\nPercept: ((5, 3),(- ,- ,- ,- ,o ))\nAction:  down\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - a \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  115\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - a \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  115\n<AGENT PROGRAM>\nPercept: ((5, 4),(- ,- ,- ,- ,o ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - a \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  115\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - a \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  115\n<AGENT PROGRAM>\nPercept: ((5, 4),(- ,- ,- ,- ,o ))\nAction:  down\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - a \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  114\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - a \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  114\n<AGENT PROGRAM>\nPercept: ((5, 5),(- ,- ,o ,- ,o ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - a \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  114\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - a \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  114\n<AGENT PROGRAM>\nPercept: ((5, 5),(- ,- ,o ,- ,o ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - a \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  114\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - a \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  114\n<AGENT PROGRAM>\nPercept: ((5, 5),(- ,- ,o ,- ,o ))\nAction:  left\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - a - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  113\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - - \n5 s - - - a - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  113\n<AGENT PROGRAM>\nPercept: ((4, 5),(- ,- ,o ,- ,- ))\nAction:  up\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h a - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  112\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h a - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  112\n<AGENT PROGRAM>\nPercept: ((4, 4),(- ,h ,- ,- ,- ))\nAction:  right\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - a \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  111\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - - \n4 - - - h - a \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  111\n<AGENT PROGRAM>\nPercept: ((5, 4),(- ,- ,- ,- ,o ))\nAction:  up\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - a \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  110\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - - a \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  110\n<AGENT PROGRAM>\nPercept: ((5, 3),(- ,- ,- ,- ,o ))\nAction:  up\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - a \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  109\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - a \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  109\n<AGENT PROGRAM>\nPercept: ((5, 2),(- ,- ,- ,- ,o ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - a \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  109\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - a \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  109\n<AGENT PROGRAM>\nPercept: ((5, 2),(- ,- ,- ,- ,o ))\nAction:  left\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w a - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  108\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w a - \n3 - w w - - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  108\n<AGENT PROGRAM>\nPercept: ((4, 2),(- ,w ,- ,- ,- ))\nAction:  down\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - a - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  107\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w - a - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  107\n<AGENT PROGRAM>\nPercept: ((4, 3),(- ,- ,- ,- ,- ))\nAction:  left\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w a - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  106\n----------------------------------------------------------------------\n<BEFORE THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w a - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  106\n<AGENT PROGRAM>\nPercept: ((3, 3),(- ,w ,h ,w ,- ))\nAction:  nop\n<AFTER THE ACTION>\n The environment content: \n\n  0 1 2 3 4 5 \n0 - - - - - - \n1 h - s w - - \n2 - - - w - - \n3 - w w a - - \n4 - - - h - - \n5 s - - - - - \n\n Agent's internal state: \n\n  0 1 2 3 4 5 \n0 g - - - - - \n1 h - s w - - \n2 - - - w - d \n3 d w w - - - \n4 - - - h d - \n5 s - - - - - \nAgent performance:  106\n----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "e = MEnv()\n",
    "\n",
    "# Isra added stuff\n",
    "w = Wall()\n",
    "g = Treasure_gold()\n",
    "d = Treasure_diamond()\n",
    "s = Snake_trap()\n",
    "h = Hole_trap()\n",
    "\n",
    "#row 0\n",
    "e.add_thing(Treasure_gold(), (0, 0))\n",
    "#row 1\n",
    "e.add_thing(Hole_trap(), (0, 1))\n",
    "e.add_thing(Snake_trap(), (2, 1))\n",
    "e.add_thing(Wall(), (3, 1))\n",
    "#row 2\n",
    "e.add_thing(Treasure_diamond(), (0, 3))\n",
    "e.add_thing(Wall(), (3, 2))\n",
    "e.add_thing(Treasure_diamond(), (5, 2))\n",
    "#row 3\n",
    "#e.add_thing(Wall(),(3,3))\n",
    "e.add_thing(Wall(), (2, 3))\n",
    "e.add_thing(Wall(), (1, 3))\n",
    "#row 4\n",
    "e.add_thing(Hole_trap(), (3, 4))\n",
    "e.add_thing(Treasure_diamond(), (4, 4))\n",
    "#row 5\n",
    "e.add_thing(Snake_trap(), (0, 5))\n",
    "\n",
    "# Isra ended adding stuff\n",
    "\n",
    "\n",
    "\n",
    "a = MAgent(program)\n",
    "#TraceAgent(a)\n",
    "e.add_thing(a, (0, 2))\n",
    "e.run(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
